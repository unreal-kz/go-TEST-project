[{"whatHappened":"The container has not passed the readiness probe.","occurrences":1,"percentage":0.00006851661527920521,"description":"Container is not ready to serve connections"},{"whatHappened":"One of the Pod's containers is not in running state.","occurrences":1,"percentage":0.00006851661527920521,"description":"Container is not running."},{"whatHappened":"One or more containers running under this Pod exited and are not in running state.","occurrences":1,"percentage":0.00006851661527920521,"description":"One or more of the Pod's containers are not running."},{"whatHappened":"The Pod is not servicing requests - probably due to the readiness probe failure.","occurrences":1,"percentage":0.00006851661527920521,"description":"Pod is not operational."},{"whatHappened":"The Pod's basic operation conditions are not met.","occurrences":1,"percentage":0.00006851661527920521,"description":"The Pod is not initialized."},{"whatHappened":"The Pod's basic operation conditions are not met.","occurrences":1,"percentage":0.00006851661527920521,"description":"The Pod is not running or not scheduled."},{"whatHappened":"The Pod's basic operation conditions are not met.","occurrences":1,"percentage":0.00006851661527920521,"description":"The Pod is not scheduled."},{"whatHappened":"The Pod's init-container failed - this will result in failure to deploy the Pod.","occurrences":1,"percentage":0.00006851661527920521,"description":"The Pod's init container did not start correctly."},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> Could not generate persistent MAC address for <<<netdeviceexact>>> No such file or directory","occurrences":1,"percentage":0.00006851661527920521,"description":"could not generate persistent mac address for netdeviceexact tried to open file or change to directory which was not found"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> Couldn't stat device <<<chardevice>>> No such file or directory","occurrences":1,"percentage":0.00006851661527920521,"description":"could not stat device chardevice tried to open file or change to directory which was not found"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<word:Created>>> slice libcontainer container <<<kubepodspodid>>> .slice.","occurrences":1,"percentage":0.00006851661527920521,"description":"created slice libcontainer container kubepodspod unique id slice."},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" <*> <*> <*> <*> <*> <*> <*> <*> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\" <<<checkingimagestatus>>> <<<code path>>> 256 <<<imagesha256>>> \" id= <<<podid2>>> name=/runtime.v1alpha2.ImageService/ImageStatus","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info checkingimagestatus code path 256 imagesha256 name runtime alpha2 imageservice imagestatus"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\" <<<checkingimagestatus>>> \" id= <<<podid2>>> name=/runtime.v1alpha2.ImageService/ImageStatus","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info checkingimagestatus name runtime alpha2 imageservice imagestatus"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info <*> container: <<<digest>>> \" id= <<<podid2>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info container digest"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info <*> <<<containerid;namespace;pod;containername>>> \" id= <<<podid2>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info container for pod in namespace"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info <*> <*> <*> <*> <*> <<<digest>>> \" id= <<<podid2>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info digest"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\" <<<image-not-found-id>>> name=/runtime.v1alpha2.ImageService/ImageStatus","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info image not found id name runtime alpha2 imageservice imagestatus"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\" <<<imagenotfound>>> <<<goobject>>> \" id= <<<podid2>>> name=/runtime.v1alpha2.ImageService/ImageStatus","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info imagenotfound goobject name runtime alpha2 imageservice imagestatus"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\"Checking image status: <<<image-format-with-space-or-quotes>>> \" id= <<<podid2>>> name=/runtime.v1alpha2.ImageService/ImageStatus","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info msg checking image status container image name runtime alpha2 imageservice imagestatus"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\"Pulling image: <<<image-format-with-space-or-quotes>>> \" id= <<<podid2>>> name=/runtime.v1alpha2.ImageService/PullImage","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info msg container image is being downloaded to the node container image name runtime alpha2 imageservice container image being downloaded"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\"Got pod network & <<<jsonobject>>> \"","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info msg got pod network jsonobject"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\"Ran pod sandbox <<<digest>>> with inf <<<namespace;pod;containername>>> \" id= <<<podid2>>> name=/runtime.v1alpha2.RuntimeService/RunPodSandbox","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info msg ran pod the container filesystem digest with inf namespace pod container name runtime alpha2 runtimeservice runpod the container filesystem"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\"Running pod sandbox: <*> id= <<<podid2>>> name=/runtime.v1alpha2.RuntimeService/RunPodSandbox","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info msg running pod the container filesystem name runtime alpha2 runtimeservice runpod the container filesystem"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\"Trying to access \\ <<<image-format-with-space-or-quotes>>> \\\"\"","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info msg trying to access container image"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\" <<<namespace;pod;containername>>> \" id= <<<podid2>>> name=/runtime.v1alpha2.RuntimeService/CreateContainer","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info namespace pod container name runtime alpha2 runtimeservice createcontainer"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info <*> pod sandbox: <<<digest>>> \" id= <<<podid2>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info pod the container filesystem digest"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=info msg=\" <<<pulledimage>>> <<<code path>>> 256 <<<imagesha256>>> \" id= <<<podid2>>> name=/runtime.v1alpha2.ImageService/PullImage","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level info pulledimage code path 256 imagesha256 name runtime alpha2 imageservice container image being downloaded"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> time=\" <<<criotime>>> Z\" level=warning msg=\"Unknown CNI cache file / <<<image>>> <<<netdeviceprecise>>> kind \\\"\\\"\"","occurrences":1,"percentage":0.00006851661527920521,"description":"criotime level warning msg container network interface cache file image netdeviceprecise kind"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ConfigMapUpdated Type:Normal Updated <<<configmapupdated;namespace>>> : cause by changes in <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace configmapupdated normal updated configmapupdated namespace cause by changes in"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Available message changed from \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\" to \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed available message changed from oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server to oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Degraded message changed from \"NodeControllerDegraded: All master nodes are ready\\nEtcdMembersDegraded: No unhealthy members found\" to \"NodeControllerDegraded: All master nodes are ready\\nClusterMemberControllerDegraded: rpc error: code = Canceled desc = grpc: the client <<<word:connection>>> is closing\\nEtcdMembersDegraded: No unhealthy members found\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed degraded message changed from node controller is not fully operational all master nodes are ready netcd cluster is not optimal no unhealthy members found to node controller is not fully operational all master nodes are ready netcd cluster is not optimal remote procedure failed canceled desc gremote procedure the client connection is closing netcd cluster is not optimal no unhealthy members found"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Degraded message changed from \"NodeControllerDegraded: All master nodes are ready\\nClusterMemberControllerDegraded: rpc error: code = Canceled desc = grpc: the client <<<word:connection>>> is closing\\nEtcdMembersDegraded: No unhealthy members found\" to \"NodeControllerDegraded: All master nodes are ready\\nEtcdMembersDegraded: No unhealthy members found\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed degraded message changed from node controller is not fully operational all master nodes are ready netcd cluster is not optimal remote procedure failed canceled desc gremote procedure the client connection is closing netcd cluster is not optimal no unhealthy members found to node controller is not fully operational all master nodes are ready netcd cluster is not optimal no unhealthy members found"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing changed from False to True (\"OAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing changed from false to true authentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing changed from True to False (\"All is well\"),Available message changed from \"OAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\" to \"OAuthServerDeploymentAvailable: availableReplicas==2\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing changed from true to false all is well available message changed from oauthserverdeploymentavailable genericnumber available replicas found for oauth server to oauthserverdeploymentavailable availablereplicas"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*> to \"OAuthVersionDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*> Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from authentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation to oauthversiondeploymentprogressing waiting for oauth server observed generation genericnumber to match expected generation waiting for oauth server observed generation genericnumber to match expected generation"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod,container is waiting in pending oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod container is waiting in pending oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 84\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\",Available message changed from \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: availableReplicas==2\" to \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 84 to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod available message changed from oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable availablereplicas to oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 84\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-667d4dc4c7-jg6nv pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 84\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 84 to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 667d4dc4c7 jg6nv pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 84"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> in oauth-openshift-667d4dc4c7-jg6nv pod,container is waiting in pending oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in oauth-openshift-667d4dc4c7-jg6nv pod,container is <*> <*> <*> oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is in oauth openshift 667d4dc4c7 jg6nv pod container is waiting in pending oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in oauth openshift 667d4dc4c7 jg6nv pod container is oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-545887685d-9cclm pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 91\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 91\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 545887685d 9cclm pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 91 to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 91"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-667d4dc4c7-jg6nv pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 84\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 84\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 667d4dc4c7 jg6nv pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 84 to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 84"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-7647d4676-cwnn5 pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 7647d4676 cwnn5 pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-85c687594b-r594l pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-85c687594b-r594l pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-85c687594b-r594l pod)\",Available message changed from \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\" to \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 85c687594b r594l pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 85c687594b r594l pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 85c687594b r594l pod available message changed from oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server to oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod,container is <*> <*> <*> <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod container is pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod,container is waiting in pending <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod container is waiting in pending pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod,container is <*> <*> <*> <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod,container is not ready in <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod container is pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod container is not ready in pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*> to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\",Available message changed from \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: availableReplicas==2\" to \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod available message changed from oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable availablereplicas to oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in oauth-openshift-667d4dc4c7-jg6nv pod,container is crashed in oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in oauth-openshift-667d4dc4c7-jg6nv pod,container is not ready in oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in oauth openshift 667d4dc4c7 jg6nv pod container is crashed in oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in oauth openshift 667d4dc4c7 jg6nv pod container is not ready in oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in oauth-openshift-667d4dc4c7-jg6nv pod,container is not ready in oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in oauth openshift 667d4dc4c7 jg6nv pod container is not ready in oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in oauth-openshift-667d4dc4c7-jg6nv pod,container is not ready in oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in oauth-openshift-667d4dc4c7-jg6nv pod,container is crashed in oauth-openshift-7647d4676-cwnn5 pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is crashed in oauth-openshift-667d4dc4c7-jg6nv pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in oauth openshift 667d4dc4c7 jg6nv pod container is not ready in oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in oauth openshift 667d4dc4c7 jg6nv pod container is crashed in oauth openshift 7647d4676 cwnn5 pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is crashed in oauth openshift 667d4dc4c7 jg6nv pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in pending oauth-openshift-85c687594b-r594l pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-85c687594b-r594l pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in pending oauth openshift 85c687594b r594l pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 85c687594b r594l pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready,"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in pending <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*> to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in pending pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-7647d4676-kn5wb pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in pending oauth-openshift-ffcf4b48-fw9hl pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-7647d4676-kn5wb pod)\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 7647d4676 kn5wb pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in pending oauth openshift ffcf4b48 fw9hl pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 7647d4676 kn5wb pod"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-7647d4676-kn5wb pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-7647d4676-kn5wb pod)\",Available message changed from \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\" to \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 7647d4676 kn5wb pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 7647d4676 kn5wb pod available message changed from oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server to oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in oauth-openshift-85c687594b-8wk6r pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\",Available message changed from \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\" to \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in oauth openshift 85c687594b 8wk6r pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, available message changed from oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server to oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in pending <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is not ready in <*> pod)\",Available message changed from \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\" to \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in pending pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is not ready in pod available message changed from oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server to oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\" to \"OAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is <*> <*> <*> <*> pod)\",Available message changed from \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\" to \"OAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod to authentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, container is pod available message changed from oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server to oauthserverdeploymentavailable genericnumber available replicas found for oauth server"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in pending oauth-openshift-85c687594b-r594l pod)\\nOAuthServerDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\",Available message changed from \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\" to \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in pending oauth openshift 85c687594b r594l pod nauthentication server deployment is progressing waiting for all oauth server replicas to be ready genericnumber not ready, available message changed from oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server to oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable genericnumber available replicas found for oauth server"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 91\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in pending oauth-openshift-545887685d-q6g2h pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 91\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 91 to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in pending oauth openshift 545887685d q6g2h pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 91"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 91\" to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, )\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation 91\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 91 to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation 91"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:OperatorStatusChanged Type:Normal Status for <<<clusteroperator>>> changed: Progressing message changed from \"OAuthVersionDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*> Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*> to \"OAuthVersionDeploymentProgressing: Waiting for all OAuth server replicas to be ready <<<genericnumber>>> not ready, container is waiting in pending <*> pod)\\nOAuthServerDeploymentProgressing: Waiting for OAuth server observed generation <<<genericnumber>>> to match expected generation <*> message changed from \"OAuthServerDeploymentAvailable: availableReplicas==2\" to \"OAuthVersionDeploymentAvailable: <<<genericnumber>>> available replicas found for OAuth Server\\nOAuthServerDeploymentAvailable: availableReplicas==2\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace kubernetes operator status changed normal status for clusteroperator changed progressing message changed from oauthversiondeploymentprogressing waiting for oauth server observed generation genericnumber to match expected generation waiting for oauth server observed generation genericnumber to match expected generation to oauthversiondeploymentprogressing waiting for all oauth server replicas to be ready genericnumber not ready, container is waiting in pending pod nauthentication server deployment is progressing waiting for oauth server observed generation genericnumber to match expected generation message changed from oauthserverdeploymentavailable availablereplicas to oauthversiondeploymentavailable genericnumber available replicas found for oauth server noauthserverdeploymentavailable availablereplicas"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> <*> Type:Normal <<<word:Created>>> <*> -n <*> because it was missing","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace normal created because it was missing"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> <*> Type:Normal Updated <*> -n <*> because it changed","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace normal updated because it changed"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ObservedConfigChanged Type:Normal Writing updated section (\"oauthServer\") of observed config: \"\\u00a0\\u00a0map[string]interface{}{\\n\\u00a0\\u00a0\\t\\\"corsAllowedOrigins\\\": []interface{}{string(`//127\\\\.0\\\\.0\\\\.1(:|$)`), string(\\\"//localhost(:|$)\\\")},\\n\\u00a0\\u00a0\\t\\\"oauthConfig\\\": map[string]interface{}{\\n\\u00a0\\u00a0\\t\\t... // <<<genericnumber>>> identical entries\\n\\u00a0\\u00a0\\t\\t\\\"loginURL\\\": string(\\\"https://api.ocp4.eu.sosiv.io:6443\\\"),\\n\\u00a0\\u00a0\\t\\t\\\"templates\\\": map[string]interface{}{\\\"error\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-ocp-branding-templ\\\"...), \\\"login\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-ocp-branding-templ\\\"...), \\\"providerSelection\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-ocp-branding-templ\\\"...)},\\n\\u00a0\\u00a0\\t\\t\\\"tokenConfig\\\": map[string]interface{}{\\n-\\u00a0\\t\\t\\t\\\"accessTokenMaxAgeSeconds\\\": <*> <*> float64(300),\\n\\u00a0\\u00a0\\t\\t},\\n\\u00a0\\u00a0\\t},\\n\\u00a0\\u00a0\\t\\\"servingInfo\\\": map[string]interface{}{\\\"cipherSuites\\\": []interface{}{string(\\\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\\\"), string(\\\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\\\"), string(\\\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\\\"), string(\\\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\\\"), ...}, \\\"minTLSVersion\\\": string(\\\"VersionTLS12\\\"), \\\"namedCertificates\\\": []interface{}{map[string]interface{}{\\\"certFile\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-router-certs/apps.\\\"...), \\\"keyFile\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-router-certs/apps.\\\"...), \\\"names\\\": []interface{}{string(\\\"*.apps.ocp4.eu.sosiv.io\\\")}}}},\\n\\u00a0\\u00a0\\t\\\"volumesToMount\\\": map[string]interface{}{\\\"identityProviders\\\": string(`{\\\"v4-0-config-user-idp-0-file-data\\\":{\\\"name\\\":\\\"htpass-secret\\\",\\\"mou`...)},\\n\\u00a0\\u00a0}\\n\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace observedconfigchanged normal writing updated section oauthserver of observed config u00a0 u00a0map string interface u00a0 corsallowedorigins interface string 127 string localhost u00a0 oauthconfig map string interface u00a0 .. genericnumber identical entries u00a0 loginurl string api ocp4 eu sosiv io 6443 u00a0 templates map string interface error string var config system secrets v4 config system ocp branding templ .. login string var config system secrets v4 config system ocp branding templ .. providerselection string var config system secrets v4 config system ocp branding templ .. u00a0 tokenconfig map string interface u00a0 accesstokenmaxageseconds float64 300 u00a0 servinginfo map string interface ciphersuites interface string tls ecdhe ecdsa with aes 128 gcm sha256 string tls ecdhe rsa with aes 128 gcm sha256 string tls ecdhe ecdsa with aes 256 gcm sha384 string tls ecdhe rsa with aes 256 gcm sha384 .. mintlsversion string versiontls12 namedcertificates interface map string interface certfile string var config system secrets v4 config system router certs apps. .. keyfile string var config system secrets v4 config system router certs apps. .. names interface string apps ocp4 eu sosiv io u00a0 volumestomount map string interface identityproviders string v4 config user idp file data name htpass secret mou .. u00a0"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ObservedConfigChanged Type:Normal Writing updated section (\"oauthServer\") of observed config: \"\\u00a0\\u00a0map[string]interface{}{\\n\\u00a0\\u00a0\\t\\\"corsAllowedOrigins\\\": []interface{}{string(`//127\\\\.0\\\\.0\\\\.1(:|$)`), string(\\\"//localhost(:|$)\\\")},\\n\\u00a0\\u00a0\\t\\\"oauthConfig\\\": map[string]interface{}{\\n\\u00a0\\u00a0\\t\\t\\\"assetPublicURL\\\": string(\\\"https://console-openshift-console.apps.ocp4.eu.sosiv.io\\\"),\\n+\\u00a0\\t\\t\\\"identityProviders\\\": []interface{}{\\n+\\u00a0\\t\\t\\tmap[string]interface{}{\\n+\\u00a0\\t\\t\\t\\t\\\"challenge\\\": bool(true),\\n+\\u00a0\\t\\t\\t\\t\\\"login\\\": bool(true),\\n+\\u00a0\\t\\t\\t\\t\\\"mappingMethod\\\": string(\\\"claim\\\"),\\n+\\u00a0\\t\\t\\t\\t\\\"name\\\": string(\\\"my_htpasswd_provider\\\"),\\n+\\u00a0\\t\\t\\t\\t\\\"provider\\\": map[string]interface{}{\\n+\\u00a0\\t\\t\\t\\t\\t\\\"apiVersion\\\": string(\\\"osin.config.openshift.io/v1\\\"),\\n+\\u00a0\\t\\t\\t\\t\\t\\\"file\\\": string(\\\"/var/config/user/idp/0/ <<<secret-direct-name>>> \\\"...),\\n+\\u00a0\\t\\t\\t\\t\\t\\\"kind\\\": string(\\\"HTPasswdPasswordIdentityProvider\\\"),\\n+\\u00a0\\t\\t\\t\\t},\\n+\\u00a0\\t\\t\\t},\\n+\\u00a0\\t\\t},\\n\\u00a0\\u00a0\\t\\t\\\"loginURL\\\": string(\\\"https://api.ocp4.eu.sosiv.io:6443\\\"),\\n\\u00a0\\u00a0\\t\\t\\\"templates\\\": map[string]interface{}{\\\"error\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-ocp-branding-templ\\\"...), \\\"login\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-ocp-branding-templ\\\"...), \\\"providerSelection\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-ocp-branding-templ\\\"...)},\\n\\u00a0\\u00a0\\t\\t\\\"tokenConfig\\\": map[string]interface{}{\\n-\\u00a0\\t\\t\\t\\\"accessTokenMaxAgeSeconds\\\": float64(172800),\\n+\\u00a0\\t\\t\\t\\\"accessTokenMaxAgeSeconds\\\": float64(86400),\\n\\u00a0\\u00a0\\t\\t\\t\\\"authorizeTokenMaxAgeSeconds\\\": float64(300),\\n\\u00a0\\u00a0\\t\\t},\\n\\u00a0\\u00a0\\t},\\n\\u00a0\\u00a0\\t\\\"servingInfo\\\": map[string]interface{}{\\\"cipherSuites\\\": []interface{}{string(\\\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\\\"), string(\\\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\\\"), string(\\\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\\\"), string(\\\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\\\"), ...}, \\\"minTLSVersion\\\": string(\\\"VersionTLS12\\\"), \\\"namedCertificates\\\": []interface{}{map[string]interface{}{\\\"certFile\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-router-certs/apps.\\\"...), \\\"keyFile\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-router-certs/apps.\\\"...), \\\"names\\\": []interface{}{string(\\\"*.apps.ocp4.eu.sosiv.io\\\")}}}},\\n-\\u00a0\\t\\\"volumesToMount\\\": map[string]interface{}{\\\"identityProviders\\\": string(\\\"{}\\\")},\\n+\\u00a0\\t\\\"volumesToMount\\\": map[string]interface{}{\\n+\\u00a0\\t\\t\\\"identityProviders\\\": string(`{\\\"v4-0-config-user-idp-0-file-data\\\":{\\\"name\\\":\\\"htpass-secret\\\",\\\"mountPath\\\":\\\"/var/config/user/idp/0/ <<<secret-direct-name>>> \\\",\\\"key\\\":\\\"htpasswd\\\",\\\"type\\\":\\\"secret\\\"}}`),\\n+\\u00a0\\t},\\n\\u00a0\\u00a0}\\n\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace observedconfigchanged normal writing updated section oauthserver of observed config u00a0 u00a0map string interface u00a0 corsallowedorigins interface string 127 string localhost u00a0 oauthconfig map string interface u00a0 assetpublicurl string console openshift console apps ocp4 eu sosiv io u00a0 identityproviders interface u00a0 tmap string interface u00a0 challenge bool true u00a0 login bool true u00a0 mappingmethod string claim u00a0 name string my htpasswd provider u00a0 provider map string interface u00a0 apiversion string osin config openshift io u00a0 file string var config user idp secret direct name .. u00a0 kind string htpasswdpasswordidentityprovider u00a0 loginurl string api ocp4 eu sosiv io 6443 u00a0 templates map string interface error string var config system secrets v4 config system ocp branding templ .. login string var config system secrets v4 config system ocp branding templ .. providerselection string var config system secrets v4 config system ocp branding templ .. u00a0 tokenconfig map string interface u00a0 accesstokenmaxageseconds float64 172800 u00a0 accesstokenmaxageseconds float64 86400 u00a0 authorizetokenmaxageseconds float64 300 u00a0 servinginfo map string interface ciphersuites interface string tls ecdhe ecdsa with aes 128 gcm sha256 string tls ecdhe rsa with aes 128 gcm sha256 string tls ecdhe ecdsa with aes 256 gcm sha384 string tls ecdhe rsa with aes 256 gcm sha384 .. mintlsversion string versiontls12 namedcertificates interface map string interface certfile string var config system secrets v4 config system router certs apps. .. keyfile string var config system secrets v4 config system router certs apps. .. names interface string apps ocp4 eu sosiv io u00a0 volumestomount map string interface identityproviders string u00a0 volumestomount map string interface u00a0 identityproviders string v4 config user idp file data name htpass secret mountpath var config user idp secret direct name key htpasswd type secret u00a0"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ObservedConfigChanged Type:Normal Writing updated section (\"oauthServer\") of observed config: \"\\u00a0\\u00a0map[string]interface{}{\\n\\u00a0\\u00a0\\t\\\"corsAllowedOrigins\\\": []interface{}{string(`//127\\\\.0\\\\.0\\\\.1(:|$)`), string(\\\"//localhost(:|$)\\\")},\\n\\u00a0\\u00a0\\t\\\"oauthConfig\\\": map[string]interface{}{\\n\\u00a0\\u00a0\\t\\t\\\"assetPublicURL\\\": string(\\\"https://console-openshift-console.apps.ocp4.eu.sosiv.io\\\"),\\n-\\u00a0\\t\\t\\\"identityProviders\\\": []interface{}{\\n-\\u00a0\\t\\t\\tmap[string]interface{}{\\n-\\u00a0\\t\\t\\t\\t\\\"challenge\\\": bool(true),\\n-\\u00a0\\t\\t\\t\\t\\\"login\\\": bool(true),\\n-\\u00a0\\t\\t\\t\\t\\\"mappingMethod\\\": string(\\\"claim\\\"),\\n-\\u00a0\\t\\t\\t\\t\\\"name\\\": string(\\\"my_htpasswd_provider\\\"),\\n-\\u00a0\\t\\t\\t\\t\\\"provider\\\": map[string]interface{}{\\n-\\u00a0\\t\\t\\t\\t\\t\\\"apiVersion\\\": string(\\\"osin.config.openshift.io/v1\\\"),\\n-\\u00a0\\t\\t\\t\\t\\t\\\"file\\\": string(\\\"/var/config/user/idp/0/ <<<secret-direct-name>>> \\\"...),\\n-\\u00a0\\t\\t\\t\\t\\t\\\"kind\\\": string(\\\"HTPasswdPasswordIdentityProvider\\\"),\\n-\\u00a0\\t\\t\\t\\t},\\n-\\u00a0\\t\\t\\t},\\n-\\u00a0\\t\\t},\\n\\u00a0\\u00a0\\t\\t\\\"loginURL\\\": string(\\\"https://api.ocp4.eu.sosiv.io:6443\\\"),\\n\\u00a0\\u00a0\\t\\t\\\"templates\\\": map[string]interface{}{\\\"error\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-ocp-branding-templ\\\"...), \\\"login\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-ocp-branding-templ\\\"...), \\\"providerSelection\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-ocp-branding-templ\\\"...)},\\n\\u00a0\\u00a0\\t\\t\\\"tokenConfig\\\": map[string]interface{}{\\n-\\u00a0\\t\\t\\t\\\"accessTokenMaxAgeSeconds\\\": float64(86400),\\n+\\u00a0\\t\\t\\t\\\"accessTokenMaxAgeSeconds\\\": float64(172800),\\n\\u00a0\\u00a0\\t\\t\\t\\\"authorizeTokenMaxAgeSeconds\\\": float64(300),\\n\\u00a0\\u00a0\\t\\t},\\n\\u00a0\\u00a0\\t},\\n\\u00a0\\u00a0\\t\\\"servingInfo\\\": map[string]interface{}{\\\"cipherSuites\\\": []interface{}{string(\\\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\\\"), string(\\\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\\\"), string(\\\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\\\"), string(\\\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\\\"), ...}, \\\"minTLSVersion\\\": string(\\\"VersionTLS12\\\"), \\\"namedCertificates\\\": []interface{}{map[string]interface{}{\\\"certFile\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-router-certs/apps.\\\"...), \\\"keyFile\\\": string(\\\"/var/config/system/secrets/v4-0-config-system-router-certs/apps.\\\"...), \\\"names\\\": []interface{}{string(\\\"*.apps.ocp4.eu.sosiv.io\\\")}}}},\\n-\\u00a0\\t\\\"volumesToMount\\\": map[string]interface{}{\\n-\\u00a0\\t\\t\\\"identityProviders\\\": string(`{\\\"v4-0-config-user-idp-0-file-data\\\":{\\\"name\\\":\\\"htpass-secret\\\",\\\"mountPath\\\":\\\"/var/config/user/idp/0/ <<<secret-direct-name>>> \\\",\\\"key\\\":\\\"htpasswd\\\",\\\"type\\\":\\\"secret\\\"}}`),\\n-\\u00a0\\t},\\n+\\u00a0\\t\\\"volumesToMount\\\": map[string]interface{}{\\\"identityProviders\\\": string(\\\"{}\\\")},\\n\\u00a0\\u00a0}\\n\"","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace observedconfigchanged normal writing updated section oauthserver of observed config u00a0 u00a0map string interface u00a0 corsallowedorigins interface string 127 string localhost u00a0 oauthconfig map string interface u00a0 assetpublicurl string console openshift console apps ocp4 eu sosiv io u00a0 identityproviders interface u00a0 tmap string interface u00a0 challenge bool true u00a0 login bool true u00a0 mappingmethod string claim u00a0 name string my htpasswd provider u00a0 provider map string interface u00a0 apiversion string osin config openshift io u00a0 file string var config user idp secret direct name .. u00a0 kind string htpasswdpasswordidentityprovider u00a0 loginurl string api ocp4 eu sosiv io 6443 u00a0 templates map string interface error string var config system secrets v4 config system ocp branding templ .. login string var config system secrets v4 config system ocp branding templ .. providerselection string var config system secrets v4 config system ocp branding templ .. u00a0 tokenconfig map string interface u00a0 accesstokenmaxageseconds float64 86400 u00a0 accesstokenmaxageseconds float64 172800 u00a0 authorizetokenmaxageseconds float64 300 u00a0 servinginfo map string interface ciphersuites interface string tls ecdhe ecdsa with aes 128 gcm sha256 string tls ecdhe rsa with aes 128 gcm sha256 string tls ecdhe ecdsa with aes 256 gcm sha384 string tls ecdhe rsa with aes 256 gcm sha384 .. mintlsversion string versiontls12 namedcertificates interface map string interface certfile string var config system secrets v4 config system router certs apps. .. keyfile string var config system secrets v4 config system router certs apps. .. names interface string apps ocp4 eu sosiv io u00a0 volumestomount map string interface u00a0 identityproviders string v4 config user idp file data name htpass secret mountpath var config user idp secret direct name key htpasswd type secret u00a0 volumestomount map string interface identityproviders string u00a0"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ObserveIdentityProviders Type:Normal identity providers changed to []","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace observeidentityproviders normal identity providers changed to"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ObserveIdentityProviders Type:Normal identity providers changed to [map[\"challenge\":%!q(bool=true) \"login\":%!q(bool=true) \"mappingMethod\":\"claim\" \"name\":\"my_htpasswd_provider\" \"provider\":map[\"apiVersion\":\"osin.config.openshift.io/v1\" \"file\":\"/var/config/user/idp/0/ <<<secret-direct-name>>> /htpasswd\" \"kind\":\"HTPasswdPasswordIdentityProvider\"]]]","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace observeidentityproviders normal identity providers changed to map challenge bool true login bool true mappingmethod claim name my htpasswd provider map apiversion osin config openshift io file var config user idp secret direct name htpasswd kind htpasswdpasswordidentityprovider"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ObserveTokenConfig Type:Normal accessTokenMaxAgeSeconds changed from <*> to <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace observetokenconfig normal accesstokenmaxageseconds changed from to"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ScalingReplicaSet Type:Normal (combined from similar events): <<<scaled-down-replica-set>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace scalingreplicaset normal combined from similar events scaled down replica set"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ScalingReplicaSet Type:Normal <<<scaled-down-replica-set>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace scalingreplicaset normal scaled down replica set"},{"whatHappened":"<<<deploymentid;deployment;namespace>>> Reason:ScalingReplicaSet Type:Normal <<<scaled-up-replica-set>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"deploymentid deployment namespace scalingreplicaset normal scaled up replica set"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <*> <<<digest>>> .scope: Consumed <*> <<<word:CPU>>> time","occurrences":1,"percentage":0.00006851661527920521,"description":"digest scope consumed cpu time"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <*> <<<digest>>> <*> Succeeded.","occurrences":1,"percentage":0.00006851661527920521,"description":"digest succeeded."},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<docker-time-stamp>>> [error] Multus: error unsetting the networks status: SetNetworkStatus: <<<word:failed>>> to query the pod <*> in out of cluster comm: <<<pod-from-event-create>>> not found","occurrences":1,"percentage":0.00006851661527920521,"description":"docker time stamp error multus error unsetting the networks status setnetworkstatus failed to query the pod in out of cluster comm pod from event create not found"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<docker-time-stamp>>> [error] SetNetworkStatus: <<<word:failed>>> to query the pod <*> in out of cluster comm: <<<pod-from-event-create>>> not found","occurrences":1,"percentage":0.00006851661527920521,"description":"docker time stamp error setnetworkstatus failed to query the pod in out of cluster comm pod from event create not found"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<docker-time-stamp>>> [verbose] Del: <*> <<<cniversion>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"docker time stamp verbose del container network interfaceversion"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<docker-time-stamp>>> [verbose] <<<namespace;pod;podid>>> <*> <<<cniversion>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"docker time stamp verbose namespace pod unique id container network interfaceversion"},{"whatHappened":"<<<endpointsid;endpoints;namespace>>> Reason:FailedToCreateEndpoint Type:Warning <<<word:Failed>>> to create <<<endpoint-namespace-and-service>>> already exists","occurrences":1,"percentage":0.00006851661527920521,"description":"endpointsid endpoints namespace failedtocreateendpoint warning failed to create endpoint namespace and service already exists"},{"whatHappened":"<<<endpointsid;endpoints;namespace>>> Reason:FailedToUpdateEndpoint Type:Warning <<<word:Failed>>> to update <<<namespace;endpoint>>> : Operation cannot be fulfilled <<<endpoints>>> : the <<<volumeobject>>> been modified; please apply your changes to the latest version and try again","occurrences":1,"percentage":0.00006851661527920521,"description":"endpointsid endpoints namespace failedtoupdateendpoint warning failed to update namespace endpoint operation cannot be fulfilled endpoints the volumeobject been modified please apply your changes to the latest version and try again"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <*> <<<genericnumber>>> flow_mods in the last <<<genericnumber>>> s <<<genericnumber>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"genericnumber flow mods in the last genericnumber"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <info> <*> manager: <*> new Open vSwitch Port device <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"info manager new open vswitch port device"},{"whatHappened":"<<<jobid;job;namespace>>> Reason:FailedCreate Type:Warning <<<errorcreatingpod>>> is forbidden: error looking up <<<namespace;serviceaccount>>> <<<serviceaccount>>> not found","occurrences":1,"percentage":0.00006851661527920521,"description":"jobid job namespace failedcreate warning errorcreatingpod is forbidden error looking up namespace serviceaccount not found"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <*> <*> <*> <*> <*> <*> <*> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel"},{"whatHappened":"<<<DateTimeutc>>> kernel[] anon <<<genericnumber>>> file <<<genericnumber>>> kernel_stack <<<genericnumber>>> slab <<<genericnumber>>> percpu <<<genericnumber>>> sock <<<genericnumber>>> shmem <<<genericnumber>>> file_mapped <<<genericnumber>>> file_dirty <<<genericnumber>>> file_writeback <<<genericnumber>>> anon_thp <<<genericnumber>>> inactive_anon <<<genericnumber>>> active_anon <<<genericnumber>>> inactive_file <<<genericnumber>>> active_file <<<genericnumber>>> unevictable <<<genericnumber>>> slab_reclaimable <<<genericnumber>>> slab_unreclaimable <<<genericnumber>>> pgfault <<<genericnumber>>> pgmajfault <<<genericnumber>>> workingset_refault_anon <<<genericnumber>>> workingset_refault_file <<<genericnumber>>> workingset_activate_anon <<<genericnumber>>> workingset_activate_file <<<genericnumber>>> workingset_restore_anon <<<genericnumber>>> workingset_restore_file <<<genericnumber>>> workingset_nodere <<<pvcclaimevent>>> pgrefill <<<genericnumber>>> pgscan <<<genericnumber>>> pgsteal <<<genericnumber>>> pgactivate <<<genericnumber>>> pgdeactivate <<<genericnumber>>> pglazyfree <<<genericnumber>>> pglazyfreed <<<genericnumber>>> thp_fault_alloc <<<genericnumber>>> thp_collapse_alloc <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel anon genericnumber file genericnumber kernel stack genericnumber slab genericnumber percpu genericnumber sock genericnumber shmem genericnumber file mapped genericnumber file dirty genericnumber file writeback genericnumber anon thp genericnumber inactive anon genericnumber active anon genericnumber inactive file genericnumber active file genericnumber unevictable genericnumber slab reclaimable genericnumber slab unreclaimable genericnumber pgfault genericnumber pgmajfault genericnumber workingset refault anon genericnumber workingset refault file genericnumber workingset activate anon genericnumber workingset activate file genericnumber workingset restore anon genericnumber workingset restore file genericnumber workingset nodere persistent volume claim pgrefill genericnumber pgscan genericnumber pgsteal genericnumber pgactivate genericnumber pgdeactivate genericnumber pglazyfree genericnumber pglazyfreed genericnumber thp fault alloc genericnumber thp collapse alloc"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<word:CPU>>> <<<genericnumber>>> <<<capitalpid>>> <<<processname>>> Not tainted <<<kernelversion>>> #1","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel cpu genericnumber capitalprocess id process name not tainted kernelversion"},{"whatHappened":"<<<DateTimeutc>>> kernel[] IPv6: <<<netdeviceaddrconf>>> link becomes ready","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel ipv6 netdeviceaddrconf link becomes ready"},{"whatHappened":"<<<DateTimeutc>>> kernel[] IPv6: <<<netdevicenetdeviceup>>> link is not ready","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel ipv6 netdevicenetdeviceup link is not ready"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<kernelcode>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel kernelcode"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<word:Memory>>> cgroup out of <<<word:memory>>> <<<killedpid;processname>>> <<<totalvm>>> , <<<anonrss>>> , <<<filerss>>> , <<<shmemrss>>> , <*> <*> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel memory control group out of memory killedprocess id process name totalvm anonphysical memory consumed filephysical memory consumed shmemphysical memory consumed"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<word:Memory>>> cgroup stats for <<<pod-from-slice>>> :","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel memory control group stats for pod from slice"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<word:Memory>>> cgroup stats for <<<pod-from-slice>>> /crio- <<<digest>>> .scope:","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel memory control group stats for pod from slice crio digest scope"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<word:memory>>> usage <*> limit <*> failcnt <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel memory usage limit failure count"},{"whatHappened":"<<<DateTimeutc>>> kernel[] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=crio- <<<digest>>> .scope,mems_allowed=0,oom_memcg= <<<pod-from-slice>>> /crio- <<<digest>>> .scope,task_memcg= <<<pod-from-slice>>> /crio- <<<digest>>> .scope, <<<processnamefromkerneltask>>> <<<pidequalsclear>>> ,uid=1000640000","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel out of memory kill constraint memory control group nodemask null cpuset crio digest scope no more memory is allowed for the process the process will probably will cause process failure and be terminated out of memory control group pod from slice crio digest scope task memory control group pod from slice crio digest scope, process name process id u1000640000"},{"whatHappened":"<<<DateTimeutc>>> kernel[] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=crio- <<<digest>>> .scope,mems_allowed=0,oom_memcg= <<<pod-from-slice>>> ,task_memcg= <<<pod-from-slice>>> /crio- <<<digest>>> .scope, <<<processnamefromkerneltask>>> <<<pidequalsclear>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel out of memory kill constraint memory control group nodemask null cpuset crio digest scope no more memory is allowed for the process the process will probably will cause process failure and be terminated out of memory control group pod from slice task memory control group pod from slice crio digest scope, process name process id"},{"whatHappened":"<<<DateTimeutc>>> kernel[] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=/,mems_allowed=0,oom_memcg= <<<pod-from-slice>>> ,task_memcg= <<<pod-from-slice>>> <*> <<<digest>>> .scope, <<<processnamefromkerneltask>>> <<<pidequalsclear>>> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel out of memory kill constraint memory control group nodemask null cpuset no more memory is allowed for the process the process will probably will cause process failure and be terminated out of memory control group pod from slice task memory control group pod from slice digest scope, process name process id"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<pid;processnameexact>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel process id process nameexact"},{"whatHappened":"<<<DateTimeutc>>> kernel[] [ pid ] uid tgid total_vm rss pgtables_bytes swapents oom_score_adj name","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel process id uid tgid total vm physical memory consumed pgtables bytes swapents out of memory score adjustment name"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<process-name-invoked-oom-killer>>> <*> <*> oom_score_adj=-1000","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel process name invoked out of memory killer out of memory score adjustment 1000"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<process-name-invoked-oom-killer>>> <*> <*> <<<oom-score-changed>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel process name invoked out of memory killer out of memory score changed"},{"whatHappened":"<<<DateTimeutc>>> kernel[] oom_reaper: <<<reapedprocess;processname>>> , now <<<anonrss>>> , <<<filerss>>> , <<<shmemrss>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel process was killed by out of memory killer due to high memory consumption causing the kernel to terminate the process agressivly terminated process name now anonphysical memory consumed filephysical memory consumed shmemphysical memory consumed"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<reg-address>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel reg address"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<reg-address>>> <<<eflags>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel reg address eflags"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<reg-address>>> <<<eflags>>> <<<origrax>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel reg address eflags origrax"},{"whatHappened":"<<<DateTimeutc>>> kernel[] <<<reg-address>>> <<<reg-address>>> <<<reg-address>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"kernel reg address reg address reg address"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<kubepodspodid>>> .slice: Consumed <*> <<<word:CPU>>> time","occurrences":1,"percentage":0.00006851661527920521,"description":"kubepodspod unique id slice consumed cpu time"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.","occurrences":1,"percentage":0.00006851661527920521,"description":"link config autonegotiation is unset or enabled, the speed and duplex are not writable."},{"whatHappened":"UID: <<<podid2>>> <*> <<<namespace-equals>>> Reason:SuccessfulCreate Type:Normal <<<pod-from-event-create2>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"namespace equals successfulcreate normal pod from event create2"},{"whatHappened":"<<<nodeid;node>>> Reason:SystemOOM Type:Warning (combined from similar events): System OOM encountered, <<<victimprocess;pid>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"nodeid node systemout of memory warning combined from similar events system out of memory encountered, victimprocess process id"},{"whatHappened":"<<<nodeid;node>>> Reason:SystemOOM Type:Warning System OOM encountered, <<<victimprocess;pid>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"nodeid node systemout of memory warning system out of memory encountered, victimprocess process id"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:AddedInterface Type:Normal Add <<<netdeviceprecise>>> [ <<<ip>>> ]","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace addedinterface normal add netdeviceprecise ip"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:BackOff Type:Normal Back-off pulling image <<<image-format-with-space-or-quotes>>> \"","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace back off normal back off container image is being downloaded to the node container image"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:BackOff Type:Warning Back-off restarting <<<word:failed>>> container","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace back off warning back off restarting failed container"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Created Type:Normal <<<created-container-name>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace created normal created container name"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:ErrImageNeverPull Type:Warning Container image <<<image-format-with-space-or-quotes>>> \" is not present with pull policy of Never","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace error image pull policy is set to never pull warning container image container image is not present with pull policy of never"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedCreatePodSandBox Type:Warning (combined from similar events): <<<word:Failed>>> to create pod sandbox: rpc error: code = Unknown desc = Kubelet may be retrying requests that are timing out in CRI-O due to system load: context deadline exceeded: error reserving pod name <<<k8s_pod;namespace;podid>>> _0 for id <<<digest>>> : name is reserved","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to create pod runtime the container filesystem environment warning combined from similar events failed to create pod the container filesystem remote procedure failed kubelet may be retrying requests that are timing out in cri due to system load expected data failed to arrive in timely manner and exceeded the deadline set as the timeout causing the operation to eventually fail error reserving pod name k8s pod in namespace for id digest name is reserved"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedCreatePodSandBox Type:Warning <<<word:Failed>>> to create pod sandbox: rpc error: code = Unknown desc = Kubelet may be retrying requests that are timing out in CRI-O due to system load: context deadline exceeded: error reserving pod name <<<k8serrornamespace;podid>>> for id <<<digest>>> : name is reserved","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to create pod runtime the container filesystem environment warning failed to create pod the container filesystem remote procedure failed kubelet may be retrying requests that are timing out in cri due to system load expected data failed to arrive in timely manner and exceeded the deadline set as the timeout causing the operation to eventually fail error reserving pod name k8serrornamespace pod unique id for id digest name is reserved"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedCreatePodSandBox Type:Warning <<<word:Failed>>> to create pod sandbox: rpc error: code = DeadlineExceeded desc = context deadline exceeded","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to create pod runtime the container filesystem environment warning failed to create pod the container filesystem remote procedure failed timeout deadline exceeded desc expected data failed to arrive in timely manner and exceeded the deadline set as the timeout causing the operation to eventually fail"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedMount Type:Warning MountVolume.SetUp <<<word:failed>>> <<<volumeforvolume>>> : <<<configmap-not-found>>> not found","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to mount disk warning mounting volume to pod setup failed volumeforvolume configmap configured for the pod does not exist"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedMount Type:Warning Unable to attach or mount volumes: unmounted <<<volumes-for-pod>>> , unattached <<<volumes-for-pod>>> : <<<word:timed>>> out waiting for the condition","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to mount disk warning unable to attach or mount volumes unmounted volumes for pod unattached volumes for pod timed out waiting for the condition"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedScheduling Type:Warning <<<NoneNodesFound>>> <<<genericnumber>>> <<<pvc-not-found>>> .","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to schedule pod on node warning could not find available node genericnumber pvc not found"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedScheduling Type:Warning <<<NoneNodesFound>>> <<<insufficient>>> <<<word:cpu>>> <<<taints>>> , that the pod didn't tolerate.","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to schedule pod on node warning could not find available node insufficient cpu found on cluster node with enough cpu to deploy this pod could not be found, taints that the pod didn tolerate."},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedScheduling Type:Warning <<<NoneNodesFound>>> <<<insufficient>>> <<<word:memory>>> <<<taints>>> , that the pod didn't tolerate.","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to schedule pod on node warning could not find available node insufficient memory found on cluster node with enough memory to deploy this pod could not be found, taints that the pod didn tolerate."},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedScheduling Type:Warning <<<NoneNodesFound>>> <<<nodesnumber>>> didn't match pod affinity rules <<<insufficientnodes>>> didn't match <<<namespace;slashpod>>> , <<<taints>>> , that the pod didn't tolerate.","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to schedule pod on node warning could not find available node nodesnumber didn match pod affinity rules insufficientnodes didn matcha pod in the namespace, taints that the pod didn tolerate."},{"whatHappened":"<<<podid;pod;namespace>>> Reason:FailedScheduling Type:Warning <<<NoneNodesFound>>> <<<nodesnumber>>> didn't match Pod's <<<node-name-endpoint>>> , <<<taints>>> , that the pod didn't tolerate.","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed to schedule pod on node warning could not find available node nodesnumber didn match pod node name endpoint taints that the pod didn tolerate."},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Failed Type:Warning Error: <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed warning failed"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Failed Type:Warning Error: <<<configmap-not-found>>> not found","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed warning failed configmap configured for the pod does not exist"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Failed Type:Warning Error: <<<word:ImagePullBackOff>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed warning failed container image could not be pulled pausing image pull retry"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Failed Type:Warning Error: <<<word:ErrImageNeverPull>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed warning failed error image pull policy is set to never pull"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Failed Type:Warning Error: <<<eventsecretnotfound>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed warning failed secret not found"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Failed Type:Warning <<<word:Failed>>> to pull image <<<image-format-with-space-or-quotes>>> \": rpc error: code = Unknown desc = <<<error-pinging-docker-registry>>> <<<headurl>>> dial tcp: <<<lookupdns>>> <<<ip;port>>> : no such host","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace failed warning failed to pull image container image remote procedure failed error pinging docker registry http url trying to open tcp connection dns resolution failed"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:InspectFailed Type:Warning <<<word:Failed>>> to apply default image tag \"<dockerRepo>/container-create-error:<release>\": couldn't parse image reference \"<dockerRepo>/container-create-error:<release>\": invalid reference format: repository name must be lowercase","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace inspectfailed warning failed to apply default image tag container repository container create failed release could not parse image reference container repository container create failed release invalid reference format repository name must be lowercase"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Killing Type:Normal <<<stoppingcontainername>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace killing normal stopping container"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Pulled Type:Normal (combined from similar events): Successfully pulled image <<<image-format-with-space-or-quotes>>> \" in <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace pulled normal combined from similar events successfully pulled image container image in"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Pulled Type:Normal Container image <<<image-format-with-space-or-quotes>>> \" already present on machine","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace pulled normal container image container image already present on machine"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Pulled Type:Normal <<<containerimage1>>> already present on machine","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace pulled normal containerimage1 already present on machine"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Pulled Type:Normal <<<containerimage1>>> <<<code path>>> 256 <<<imagesha256>>> \" already present on machine","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace pulled normal containerimage1 code path 256 imagesha256 already present on machine"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Pulled Type:Normal Successfully pulled image <<<image-format-with-space-or-quotes>>> \" in <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace pulled normal successfully pulled image container image in"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Pulled Type:Normal Successfully <<<pulledimage>>> in <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace pulled normal successfully pulledimage in"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Pulling Type:Normal Pulling image <<<image-format-with-space-or-quotes>>> \"","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace pulling normal container image is being downloaded to the node container image"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Pulling Type:Normal <<<imagepull>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace pulling normal imagepull"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Scheduled Type:Normal Successfully <<<namespace;pod;node>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace scheduled normal successfully pod in namespace on node"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Started Type:Normal <<<containerstarted>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace started normal containerstarted"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Unhealthy Type:Warning <<<livenessprobe-catch>>> <<<word:failed>>> <<<headurl>>> dial tcp <<<ip;port>>> : connect: <<<word:connection>>> <<<word:refused>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace unhealthy warning livenessprobe probe failed http urlopen connection to the ip and port network connection attempted connection refused"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Unhealthy Type:Warning <<<readinessprobe-catch>>> <<<word:failed>>> <<<headurl>>> net/http: request canceled while waiting for <<<word:connection>>> (Client.Timeout exceeded while awaiting headers)","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace unhealthy warning readiness probe failed http url net http request canceled while waiting for connection http client failed due to timeout while awaiting for response from the server"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Unhealthy Type:Warning <<<readinessprobe-catch>>> <<<word:failed>>> <<<headurl>>> read tcp <<<ip;port;ip;portprecise>>> : read: <<<word:connection>>> reset by peer","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace unhealthy warning readiness probe failed http url read tcp ip port ip port read connection reset by peer"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Unhealthy Type:Warning <<<readinessprobe-catch>>> <<<word:failed>>> <<<headurl>>> dial tcp <<<ip;port>>> : connect: <<<word:connection>>> <<<word:refused>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace unhealthy warning readiness probe failed http urlopen connection to the ip and port network connection attempted connection refused"},{"whatHappened":"<<<podid;pod;namespace>>> Reason:Unhealthy Type:Warning <<<readinessprobe-catch>>> <<<word:failed>>> <<<probe-failed-500>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"pod in namespace unhealthy warning readiness probe failed probe failed with http 500 response"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<word:Removed>>> slice libcontainer container <<<kubepodspodid>>> .slice.","occurrences":1,"percentage":0.00006851661527920521,"description":"removed slice libcontainer container kubepodspod unique id slice."},{"whatHappened":"<<<replicasetid;replicaset;namespace>>> Reason:FailedCreate Type:Warning <<<errorcreatingpod>>> is forbidden: error looking up <<<namespace;serviceaccount>>> <<<serviceaccount>>> not found","occurrences":1,"percentage":0.00006851661527920521,"description":"replicasetid replicaset namespace failedcreate warning errorcreatingpod is forbidden error looking up namespace serviceaccount not found"},{"whatHappened":"<<<replicasetid;replicaset;namespace>>> Reason:FailedCreate Type:Warning <<<errorcreatingpod>>> is forbidden: no PriorityClass with name high-priority was found","occurrences":1,"percentage":0.00006851661527920521,"description":"replicasetid replicaset namespace failedcreate warning errorcreatingpod is forbidden no priorityclass with name high priority was found"},{"whatHappened":"<<<replicasetid;replicaset;namespace>>> Reason:FailedCreate Type:Warning <<<errorcreatingpod>>> is forbidden: unable to validate against any security context constraint: [ <<<sourceprocess>>> .securityContext.runAsUser: Invalid value: <<<user-id-not-in-allowed-range>>> ]","occurrences":1,"percentage":0.00006851661527920521,"description":"replicasetid replicaset namespace failedcreate warning errorcreatingpod is forbidden unable to validate against any security context constraint securitycontext runasuser invalid value user id not in allowed range"},{"whatHappened":"<<<replicasetid;replicaset;namespace>>> Reason:SuccessfulCreate Type:Normal (combined from similar events): <<<pod-from-event-create2>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"replicasetid replicaset namespace successfulcreate normal combined from similar events pod from event create2"},{"whatHappened":"<<<replicasetid;replicaset;namespace>>> Reason:SuccessfulCreate Type:Normal <<<pod-from-event-create2>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"replicasetid replicaset namespace successfulcreate normal pod from event create2"},{"whatHappened":"<<<replicasetid;replicaset;namespace>>> Reason:SuccessfulDelete Type:Normal <<<deleted-pod>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"replicasetid replicaset namespace successfuldelete normal deleted pod"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <*> <*> <*> <*> <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Cleaned up orphaned pod volumes dir from pod \" <<<podid2>>> \" at /var/lib/kubelet/pods/ <<<podid2>>> /volumes","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile cleaned up orphaned pod volumes dir from pod at containers sandbox mount on the kubernetes node volumes"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Container \" <<<digest>>> \" not found in pod's containers","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container digest not found in pod containers"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> PullImage <<<image-format-with-space-or-quotes>>> \" from image service <<<word:failed>>> rpc error: code = Unknown desc = <<<error-pinging-docker-registry>>> <<<headurl>>> dial tcp: <<<lookupdns>>> <<<ip;port>>> : no such host","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container image being downloaded container image from image service failed remote procedure failed error pinging docker registry http url trying to open tcp connection dns resolution failed"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> PullImage <<<image-format-with-space-or-quotes>>> \" from image service <<<word:failed>>> rpc error: code = Unknown desc = <<<errorreadingmanifest>>> unauthorized: access to the requested resource is not authorized","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container image being downloaded container image from image service failed remote procedure failed errorreadingmanifest not authorized access to the requested resource is not authorized"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> PullImage <<<image-format-with-space-or-quotes>>> \" from image service <<<word:failed>>> rpc error: code = Unknown desc = <<<imageerrorreadingmanifest>>> unknown: manifest unknown","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container image being downloaded container image from image service failed remote procedure failed imageerrorreadingcontainer image manifest was not found in the container repository container image manifest was not found in the container repository"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> container <<<containerobject>>> start <<<word:failed>>> in pod <<<pod;namespace;podid>>> : <<<word:CreateContainerConfigError>>> <<<configmap-not-found>>> not found","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container start failed in pod in namespace creating container configuration failed, either secret persistent volume or other pod configuration failed configmap configured for the pod does not exist"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> container <<<containerobject>>> start <<<word:failed>>> in pod <<<pod;namespace;podid>>> : ErrImagePull: rpc error: code = Unknown desc = <<<error-pinging-docker-registry>>> <<<headurl>>> dial tcp: <<<lookupdns>>> <<<ip;port>>> : no such host","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container start failed in pod in namespace error downloading container image remote procedure failed error pinging docker registry http url trying to open tcp connection dns resolution failed"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> container <<<containerobject>>> start <<<word:failed>>> in pod <<<pod;namespace;podid>>> : ErrImagePull: rpc error: code = Unknown desc = <<<errorreadingmanifest>>> unauthorized: access to the requested resource is not authorized","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container start failed in pod in namespace error downloading container image remote procedure failed errorreadingmanifest not authorized access to the requested resource is not authorized"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> container <<<containerobject>>> start <<<word:failed>>> in pod <<<pod;namespace;podid>>> : ErrImagePull: rpc error: code = Unknown desc = <<<errorreadingmanifest>>> unauthorized: authentication required","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container start failed in pod in namespace error downloading container image remote procedure failed errorreadingmanifest not authorized authentication required"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> container <<<containerobject>>> start <<<word:failed>>> in pod <<<pod;namespace;podid>>> : ErrImagePull: rpc error: code = Unknown desc = <<<imageerrorreadingmanifest>>> unknown: manifest unknown","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container start failed in pod in namespace error downloading container image remote procedure failed imageerrorreadingcontainer image manifest was not found in the container repository container image manifest was not found in the container repository"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> container <<<containerobject>>> start <<<word:failed>>> in pod <<<pod;namespace;podid>>> : <<<word:ErrImageNeverPull>>> Container image <<<image-format-with-space-or-quotes>>> \" is not present with pull policy of Never","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container start failed in pod in namespace error image pull policy is set to never pull container image container image is not present with pull policy of never"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> container <<<containerobject>>> start <<<word:failed>>> in pod <<<pod;namespace;podid>>> : InvalidImageName: <<<word:Failed>>> to apply default image tag \"<dockerRepo>/container-create-error:<release>\": couldn't parse image reference \"<dockerRepo>/container-create-error:<release>\": invalid reference format: repository name must be lowercase","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile container start failed in pod in namespace image failed to apply default image tag container repository container create failed release could not parse image reference container repository container create failed release invalid reference format repository name must be lowercase"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Could not get instant <<<word:cpu>>> stats: cumulative stats decrease","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile could not get instant cpu stats cumulative stats decrease"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Couldn't get <<<namespace;configmapprecisename>>> : <<<configmap-not-found>>> not found","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile could not get namespace configmapprecisename configmap configured for the pod does not exist"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> RemoveContainer \" <<<digest>>> \" from runtime service <<<word:failed>>> rpc error: code = Unknown desc = no such id: ' <<<digest>>> '","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile deleting the container digest from runtime service failed remote procedure failed no such id digest"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Event(v1.ObjectReference{Kind:\"Pod\", <<<namespace-equals>>> , <*> UID:\" <<<podid2>>> \", APIVersion:\"v1\", <*> FieldPath:\"\"}): type: 'Normal' reason: 'AddedInterface' Add <<<netdeviceprecise>>> [ <<<ip>>> ]","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile event objectreference kind pod namespace equals apiversion fieldpath normal addedinterface add netdeviceprecise ip"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<word:failed>>> to collect filesystem stats - rootDiskErr: could not stat \"/ <<<image>>> /diff\" to get i <<<node-name-endpoint>>> : stat / <<<image>>> /diff: no such file or directory, extraDiskErr: could not stat \" <<<log-file>>> \" to get i <<<node-name-endpoint>>> : stat <<<log-file>>> : no such file or directory","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile failed to collect filesystem stats rootdiskerr could not stat image diff to get node name endpoint stat image diff tried to open file or change to directory which was not found, extradiskerr could not stat log file to get node name endpoint stat log file tried to open file or change to directory which was not found"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<word:failed>>> to delete cgroup paths for [kubepods burstable pod <<<podid2>>> ] : unable to destroy cgroup paths for cgroup [kubepods burstable pod <<<podid2>>> ] : <<<word:Failed>>> to remove paths: map[blkio:/sys/fs/cgroup/blkio <<<pod-from-slice>>> cpu:/sys/fs/cgroup/cpu,cpuacct <<<pod-from-slice>>> cpuacct:/sys/fs/cgroup/cpu,cpuacct <<<pod-from-slice>>> cpuset:/sys/fs/cgroup/cpuset <<<pod-from-slice>>> devices:/sys/fs/cgroup/devices <<<pod-from-slice>>> freezer:/sys/fs/cgroup/freezer <<<pod-from-slice>>> hugetlb:/sys/fs/cgroup/hugetlb <<<pod-from-slice>>> memory:/sys/fs/cgroup/memory <<<pod-from-slice>>> net_cls:/sys/fs/cgroup/net_cls,net_prio <<<pod-from-slice>>> net_prio:/sys/fs/cgroup/net_cls,net_prio <<<pod-from-slice>>> perf_event:/sys/fs/cgroup/perf_event <<<pod-from-slice>>> pids:/sys/fs/cgroup/pids <<<pod-from-slice>>> systemd:/sys/fs/cgroup/systemd <<<pod-from-slice>>> ]","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile failed to delete control group paths for kubepods burstable pod unable to destroy control group paths for control group kubepods burstable pod failed to remove paths map blkio sys fs control group blkio pod from slice cpu sys fs control group cpu cpuacct pod from slice cpuacct sys fs control group cpu cpuacct pod from slice cpuset sys fs control group cpuset pod from slice devices sys fs control group devices pod from slice freezer sys fs control group freezer pod from slice hugetlb sys fs control group hugetlb pod from slice memory sys fs control group memory pod from slice net cls sys fs control group net cls net prio pod from slice net prio sys fs control group net cls net prio pod from slice perf event sys fs control group perf event pod from slice process ids sys fs control group process ids pod from slice systemd sys fs control group systemd pod from slice"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<word:failed>>> to remove pod init container \"wait-for-airflow-migrations\": rpc error: code = Unknown desc = no such id: ' <<<digest>>> '; Skipping pod \" <<<pod;namespace;podid>>> \"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile failed to remove pod init container wait for airflow migrations remote procedure failed no such id digest skipping pod in namespace"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<word:Failed>>> to update stats for container \" <<<pod-from-slice>>> /crio- <<<digest>>> .scope\": unable to determine device info for dir: / <<<image>>> /diff: stat <<<word:failed>>> on / <<<image>>> /diff with error: no such file or directory, continuing to push stats","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile failed to update stats for container pod from slice crio digest scope unable to determine device info for dir image diff stat failed on image diff with failed tried to open file or change to directory which was not found, continuing to push stats"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Got sys oom event: <*> <*> <<<DateTimeutc>>> <<<mplusignore>>> / / }","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile got sys out of memory event mplusignore"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Killing container \"cri-o:// <<<digest>>> \" with a <<<genericnumber>>> second grace period","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile killing container cri digest with genericnumber second grace period"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Killing unwanted <<<pod-from-event-create>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile killing unwanted pod from event create"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> [topologymanager] RemoveContainer - <<<containeridcolon>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile kubernetes node topology manager deleting the container containeridcolon"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> operationExecutor.MountVolume <<<word:started>>> <<<volumeforvolume>>> (UniqueName: \"kubernetes.io/ <<<configmapevent>>> \") <<<pod-from-event-create>>> (UID: \" <<<podid2>>> \")","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile mounting volume to pod started volumeforvolume kubernetes io configmapevent pod from event create"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> No sandbox for pod \" <<<pod;namespace;podid>>> \" can be found. Need to start a new one","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile no the container filesystem for pod in namespace can be found. need to start new one"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Operation for \"{volumeName:kubernetes.io/ <<<configmapevent>>> podName: <<<podid2>>> nodeName:}\" <<<word:failed>>> No retries permitted until <<<DateTimeutc>>> <<<mplusignore>>> (durationBeforeRetry 2m2s). Error: \"MountVolume.SetUp <<<word:failed>>> for volume <*> (UniqueName: \\\"kubernetes.io/ <<<configmapevent>>> \\\") <<<pod-from-event-create>>> (UID: \\\" <<<podid2>>> \\\") : <<<configmap-not-found>>> not found\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile operation for volume name kubernetes io configmapevent pod node failed. no retries permitted until mplusignore duration failed mounting volume to pod setup failed for volume kubernetes io configmapevent pod from event create configmap configured for the pod does not exist"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Operation for \"{volumeName:kubernetes.io/ <<<configmapevent>>> podName: <<<podid2>>> nodeName:}\" <<<word:failed>>> No retries permitted until <<<DateTimeutc>>> <<<mplusignore>>> (durationBeforeRetry 2m2s). Error: \"UnmountVolume.TearDown <<<word:failed>>> for volume \\\"serving-certs-ca-bundle\\\" (UniqueName: \\\"kubernetes.io/ <<<configmapevent>>> \\\") pod \\\" <<<podid2>>> \\\" (UID: \\\" <<<podid2>>> \\\") : unlinkat /var/lib/kubelet/pods/ <<<podid2>>> /volumes/kubernetes.io~ <<<configmapevent>>> /..2022_04_06_04_51_28.380056643: directory not empty\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile operation for volume name kubernetes io configmapevent pod node failed. no retries permitted until mplusignore duration failed unmounting volume before deleting container failed for volume serving certs ca bundle kubernetes io configmapevent pod unlinkat containers sandbox mount on the kubernetes node volumes kubernetes io configmapevent .2022 04 06 04 51 28 380056643 directory not empty"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Path \"/var/lib/kubelet/pods/ <<<podid2>>> /volumes\" does not exist","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile path containers sandbox mount on the kubernetes node volumes does not exist"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: <<<word:failed>>> <<<tostartcontainerfordeployment>>> with <<<word:ImagePullBackOff>>> \"Back-off pulling image \\ <<<image-format-with-space-or-quotes>>> \\\"\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping failed tostart container commandfordeployment with container image could not be pulled pausing image pull retry back off container image is being downloaded to the node container image"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: <<<word:failed>>> <<<tostartcontainerfordeployment>>> with <<<word:CreateContainerConfigError>>> \" <<<configmap-not-found>>> not found\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping failed tostart container commandfordeployment with creating container configuration failed, either secret persistent volume or other pod configuration failed configmap configured for the pod does not exist"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: <<<word:failed>>> <<<tostartcontainerfordeployment>>> with ErrImagePull: \"rpc error: code = Unknown desc = <<<error-pinging-docker-registry>>> <<<headurl>>> dial tcp: <<<lookupdns>>> <<<ip;port>>> : no such host\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping failed tostart container commandfordeployment with error downloading container image remote procedure failed error pinging docker registry http url trying to open tcp connection dns resolution failed"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: <<<word:failed>>> <<<tostartcontainerfordeployment>>> with ErrImagePull: \"rpc error: code = Unknown desc = <<<errorreadingmanifest>>> unauthorized: access to the requested resource is not authorized\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping failed tostart container commandfordeployment with error downloading container image remote procedure failed errorreadingmanifest not authorized access to the requested resource is not authorized"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: <<<word:failed>>> <<<tostartcontainerfordeployment>>> with ErrImagePull: \"rpc error: code = Unknown desc = <<<imageerrorreadingmanifest>>> unknown: manifest unknown\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping failed tostart container commandfordeployment with error downloading container image remote procedure failed imageerrorreadingcontainer image manifest was not found in the container repository container image manifest was not found in the container repository"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: <<<word:failed>>> <<<tostartcontainerfordeployment>>> with <<<word:ErrImageNeverPull>>> \"Container image \\ <<<image-format-with-space-or-quotes>>> \\\" is not present with pull policy of Never\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping failed tostart container commandfordeployment with error image pull policy is set to never pull container image container image is not present with pull policy of never"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: <<<word:failed>>> <<<tostartcontainerfordeployment>>> with InvalidImageName: \"Failed to apply default image tag \\\"<dockerRepo>/container-create-error:<release>\\\": couldn't parse image reference \\\"<dockerRepo>/container-create-error:<release>\\\": invalid reference format: repository name must be lowercase\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping failed tostart container commandfordeployment with image failed to apply default image tag container repository container create failed release could not parse image reference container repository container create failed release invalid reference format repository name must be lowercase"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: [failed <<<tostartcontainerfordeployment>>> with <<<word:CrashLoopBackOff>>> \"back-off 5m0s restarting <<<word:failed>>> <<<container-equals>>> pod= <<<pod;namespace;podid>>> \", <<<word:failed>>> <<<tostartcontainerfordeployment>>> with <<<word:CrashLoopBackOff>>> \"back-off 5m0s restarting <<<word:failed>>> <<<container-equals>>> pod= <<<pod;namespace;podid>>> \"]","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping failed tostart container commandfordeployment with the container does not run properly and is failing continuously. kubernetes will delay the next restart of the container back off 5m0s restarting failed container pod in namespace failed tostart container commandfordeployment with the container does not run properly and is failing continuously. kubernetes will delay the next restart of the container back off 5m0s restarting failed container pod in namespace"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: <<<word:failed>>> <<<tostartcontainerfordeployment>>> with <<<word:CrashLoopBackOff>>> \"back-off <*> restarting <<<word:failed>>> <<<container-equals>>> pod= <<<pod;namespace;podid>>> \"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping failed tostart container commandfordeployment with the container does not run properly and is failing continuously. kubernetes will delay the next restart of the container back off restarting failed container pod in namespace"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Error syncing pod <<<podid2>>> (\" <<<pod;namespace;podid>>> \"), skipping: unmounted <<<volumes-for-pod>>> , unattached <<<volumes-for-pod>>> : <<<word:timed>>> out waiting for the condition","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod could not reach running phase pod in namespace skipping unmounted volumes for pod unattached volumes for pod timed out waiting for the condition"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> \"Pod status updated\" <<<namespace-pod-eq>>> \" <*>","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pod status updated namespace"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Pull image <<<image-format-with-space-or-quotes>>> \" <<<word:failed>>> rpc error: code = Unknown desc = <<<error-pinging-docker-registry>>> <<<headurl>>> dial tcp: <<<lookupdns>>> <<<ip;port>>> : no such host","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pull image container image failed remote procedure failed error pinging docker registry http url trying to open tcp connection dns resolution failed"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Pull image <<<image-format-with-space-or-quotes>>> \" <<<word:failed>>> rpc error: code = Unknown desc = <<<errorreadingmanifest>>> unauthorized: access to the requested resource is not authorized","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile pull image container image failed remote procedure failed errorreadingmanifest not authorized access to the requested resource is not authorized"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<word:Starting>>> reflector <*> (0s) from <<<object-namespace>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile start caching entity state 0s from object namespace"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Stopping reflector <*> (0s) from <<<object-namespace>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile stop caching entity state 0s from object namespace"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<SyncLoop>>> \" <<<pod;namespace;podid>>> \"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile syncloop pod in namespace"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<SyncLoop>>> \" <<<pod;namespace;podid>>> \", event: &pleg.PodLifecycleEvent{ID:\" <<<podid2>>> \", <*> Data:\" <<<digest>>> \"}","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile syncloop pod in namespace event pod lifecycle event generator ppod lifecycle event id data digest"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Unable to attach or mount volumes for pod \" <<<pod;namespace;podid>>> \": unmounted <<<volumes-for-pod>>> , unattached <<<volumes-for-pod>>> : <<<word:timed>>> out waiting for the condition; skipping pod","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile unable to attach or mount volumes for pod in namespace unmounted volumes for pod unattached volumes for pod timed out waiting for the condition skipping pod"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Unable to retrieve <<<namespace;pullsecret;namespace;pod>>> due to <<<eventsecretnotfound>>> . The image pull may not succeed.","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile unable to retrieve namespace pullsecret namespace pod due to secret not found container image download from the repository will fail."},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<unable-to-retrieve-pull-secret>>> for <*> <<<due-to-secret-not-found>>> not found. The image pull may not succeed.","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile unable to retrieve pull secret for due to secret not found not found. container image download from the repository will fail."},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> UnmountVolume.TearDown succeeded for volume \" <<<secretkubernetesio>>> \" ( <<<outervolumespacename>>> ) pod \" <<<podid2>>> \" (UID: \" <<<podid2>>> \"). <<<innervolumespacename>>> . PluginName \"kubernetes.io/secret\", VolumeGidValue \"\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile unmounting volume before deleting container succeeded for volume secretkubernetesio outervolumespacename pod innervolumespacename pluginname kubernetes io secret volumegidvalue"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<verify-volume-attachment>>> <<<volumestartedsucceeded>>> (UniqueName: \" <<<secretkubernetesio>>> \") <<<pod-from-event-create>>> (UID: \" <<<podid2>>> \")","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile verifying that the volume is attached volumestartedsucceeded secretkubernetesio pod from event create"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Volume detached <<<volumeforvolume>>> (UniqueName: \" <<<secretkubernetesio>>> \") <<<onnodequotes>>> DevicePath \"\"","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile volume detached volumeforvolume secretkubernetesio onnodequotes devicepath"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<volume-unmount>>> <<<word:started>>> <<<volumeforvolume>>> (UniqueName: \"kubernetes.io/ <<<configmapevent>>> \") pod \" <<<podid2>>> \" (UID: \" <<<podid2>>> \")","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile volume unmount started volumeforvolume kubernetes io configmapevent pod"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <<<volume-unmount>>> <<<volumestartedsucceeded>>> (UniqueName: \" <<<secretkubernetesio>>> \") pod \" <<<podid2>>> \" (UID: \" <<<podid2>>> \")","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile volume unmount volumestartedsucceeded secretkubernetesio pod"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> <*> <<<volumestartedsucceeded>>> (UniqueName: \" <<<secretkubernetesio>>> \") <<<pod-from-event-create>>> (UID: \" <<<podid2>>> \")","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile volumestartedsucceeded secretkubernetesio pod from event create"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<severity-Level>>> <<<time>>> <<<gofile>>> Warning: <<<word:Failed>>> to clear quota on /var/lib/kubelet/pods/ <<<podid2>>> /volumes/kubernetes.io~ <<<configmapevent>>> : clearQuota called, but quotas disabled","occurrences":1,"percentage":0.00006851661527920521,"description":"severity level time gofile warning kubernetes tried to clear the quote during volume removal process and failed. most likely caused due to quotas being disabled. on containers sandbox mount on the kubernetes node volumes kubernetes io configmapevent clear quota remove disk quota for volume. called, but quotas disabled"},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<word:Started>>> crio-conmon- <<<digest>>> .scope.","occurrences":1,"percentage":0.00006851661527920521,"description":"started crio conmon digest scope."},{"whatHappened":"<<<DateTimeutc>>> <<<sourceprocess>>> <<<word:Started>>> libcontainer <<<containeridspaceendline>>>","occurrences":1,"percentage":0.00006851661527920521,"description":"started libcontainer container"}]
